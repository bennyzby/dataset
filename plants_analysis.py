# -*- coding: utf-8 -*-
"""Exam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11N-XBlqRm9xT_R2By4otvnNkhszulrYh

# **Dataset and Cleaning**
"""

import pandas as pd
import numpy as np

df = pd.read_excel('https://xiaoxl.github.io/ml22/_downloads/cf9f1887499f2b7a093a867bf253ad56/plants.xlsx', engine='openpyxl', sheet_name='data')

df['Outcome_after 12 months'].fillna('dead', inplace=True)
df = df.dropna()
df = df.drop(columns=['SN'],axis=1)
df = pd.get_dummies(df, columns=['Tree_Replicate'])
df

from sklearn.model_selection import train_test_split

df['Outcome_after 12 months']=np.array(df['Outcome_after 12 months'].replace(['survived','dead'],[0,1]),dtype=int)
df['Treatment']=np.array(df['Treatment'].replace(['Salt','Fresh'],[0,1]),dtype=int)
df['Endophyte ']=np.array(df['Endophyte '].replace(['I+','I-'],[0,1]),dtype=int)

X = np.array(df[['Endophyte ','Treatment', 'Salt Orgins ','Leaf_number','height','node number','Branching','Stem diameter','Tree_Replicate_T1','Tree_Replicate_T2','Tree_Replicate_T3']])#SN is not include in the model
y = np.array(df['Outcome_after 12 months'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
df

"""# Pair Plot

Since this dataset have more than two features, and we use pairplot.
"""

import seaborn as sns
sns.pairplot(data=df, hue='Outcome_after 12 months')

"""In the pairplot, we can find that dead cases is more than survived cases. It seems that SN, Salt Orgins, height, Treatment, Stem diameter and Leaf_number are better than other features."""

sns.set_theme()
sns.relplot(x='height', y='Treatment', hue='Outcome_after 12 months',style='Outcome_after 12 months' ,data=df, kind='scatter',aspect=16/9, height=5)

"""# kNN"""

# Using Sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score

n_neighbors = 10
steps = [('scaler', MinMaxScaler()),
         ('knn', KNeighborsClassifier(n_neighbors, weights="uniform",
                                      metric="euclidean", algorithm='brute'))]
pipe = Pipeline(steps=steps)
pipe.fit(X_train, y_train)
y_pipe = pipe.predict(X_test)
print(accuracy_score(y_pipe, y_test))

"""We can see the accuracy score of this model is 76.56%

Choosing the K value
"""

# Using GridSearchCV model to choose one k value be used in our model
from sklearn.model_selection import GridSearchCV, cross_val_score
n_list = list(range(1, 101))
parameters = dict(knn__n_neighbors=n_list)
clf = GridSearchCV(pipe, parameters)
clf.fit(X, y)
print(clf.best_estimator_.get_params()["knn__n_neighbors"])

"""The best K value in this case is 7"""

cv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)
print(np.mean(cv_scores))

"""From this result, in this case the best k is 7. The corresponding cross-validation score is 73.98%"""

importance = grid_search_cv.best_estimator_.feature_importances_
for i, V in enumerate(importance):
  print('Feature: %0d, Score:%5f' % (i,V))
plt.bar([x for x in range(len(importance))], importance)
plt.show()

"""# Decision Tree"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

params = {'min_samples_split': list(range(2, 5)),
          'max_leaf_nodes': list(range(2, 50))}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), 
                              params, verbose=1, cv=3)
#Use grid search find the best min sample split and max leaf nodes
grid_search_cv.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

clf = grid_search_cv.best_estimator_
#Put the best parameter in clf model
print(grid_search_cv.best_params_)
y_pred = clf.predict(X_test)
accuracy_score(y_pred, y_test)
#calculate the accuracy rate about this model

"""Now you can see that for this plants dataset, the best decision tree should have at most 8 leaf nodes and the minimum number of samples required to be at a leaft node is 2. The fitted decision tree can get 60.94% accuracy on the test set.

Plot the Decision Tree
"""

from sklearn import tree
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 15), dpi=300)
tree.plot_tree(clf, filled=True)
plt.show

"""In the plot of Deciosn Tree, you can see that there are only use 5 importmant features in the mdoel.

# **Conclusion**
Depends on the accuracy, I prefer to use kNN model in this dataset.
"""